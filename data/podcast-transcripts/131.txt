00:00:00,080 --> 00:00:19,990 [Speaker 0]
Hey, everyone. Thank you so much for watching another episode of the Weaviate Podcast. I'm super excited to welcome Matthew Russo, a PhD student at MIT. Matthew is doing incredible work at the intersection of AI and database systems, including his recent work on SemBench, a benchmark for semantic query processing engines. I'm super excited to dive into these things. Matthew, thanks so much for joining the podcast. 

00:00:19,990 --> 00:00:22,000 [Speaker 1]
Yeah, thank you for having me. 

00:00:22,000 --> 00:00:33,060 [Speaker 0]
Awesome. So I think I'd love to kick it off for our audience to just kind of explain this concept of a semantic query processing engine, and maybe introduce semantic operators and just sort of, uh, set the scene for this. 

00:00:33,060 --> 00:03:33,340 [Speaker 1]
Yeah, totally. Um, so in general, uh, I think the way, the best way that you can think of this pitch is that imagine that you have, you know, your traditional database, um, which has a relational algebra. You use SQL to query it, uh, and that gives you access to a bunch of, uh, operators under the hood such as filters, joins, aggregates, groupbys, and so on. And just simply put, you know, if you try to issue a SQL query over, let's say, a database that has, um, you know, let's say pointers to images of cars. And you say, "Okay, I want a filter for every row in my database that has an image of a red car." Well, currently this is not, like, really supported. Um, you know, your, your traditional relational operator has no way of, uh, computing this filter, the car in the image that's linked by this column's file path, um, is, has the color red. But today with LMS, uh, you know, this is actually suddenly very possible. And, y- in the past there had been some work, um, in the database's community to try and extend sort of the SQL relational model to include these types of, um, you know, image and video-based filters, uh, text-based filters as well by sort of imp- putting these MLUDFs into the database. But what's really changed now with the foundation models that we have both in terms of language models and vision models, is that they have extremely good zero-shot capability, right? So in the past, you kind of had to optimize the UDF for like, you know, you probably had to train a cascade model and tune it and parameterize it so that for that one query about red cars, it would do a really good job at filtering for red cars in a way that was, like, fast and, you know, high in terms of accuracy. But the great thing is that now that we have, uh, you know, these vision models, you could just ask it on one query, "You know, I want a filter for red cars." The next query could be, "You know, I want a filter for Porsche 911s." Uh, and because of sort of their, you know, their general world and semantic understanding, it's possible to just build a system around these foundation and language models that is able to sort of in zero-shot just answer any query that has some of these semantic operators. And so perhaps to just be very explicit, what I mean when we talk about semantic operators is you can imagine that you can specify, um, the where clause in a SQL query, uh, which we would say is a semantic filter, um, using just a natural language sentence, so, "The color of the car is red." You can also specify, uh, join predicates, um, in natural language. So I think in SemBench, one of the workloads, we have, um, audio recordings of animal noises as well as video... Uh, or taken as images capturing where these, uh, animals are by, uh, you know, trap recordings. And you can effectively say, "You know, I want to f- I want to join every instance of audio where that audio is the same animal as in the image." Right? So if you hear, like, the elephant trumpet and you see a ph- a photo of an elephant, you hope that those two, you know- 

00:03:33,340 --> 00:03:33,470 [Speaker 0]
Mm-hmm 

00:03:33,470 --> 00:05:46,010 [Speaker 1]
... each audio pair gets joined together. Um, and then, you know, in some of my past work, um, we've done a lot of work as well with optimizing, uh, semantic maps. So this is something that you... I- it sort of shows up in databases in the sense that you can, like, create new columns on the fly, um, using, you know, t- traditional, uh, relational algebra and o- other operations. But, um, specifically here, for instance, we can have a dataset of, uh, research papers and we can apply a semantic map over the research papers to say, like, you know, "Summarize the paper." Compute sort of like a summary field. Um, and so the list goes on. There's also semantic aggregates, um, you know, semantic groupbys as well where maybe you're coming up with these groups on the fly, like, "What are my customers' top three complaints about some product," right? You need some semantic understanding to figure out what are the complaint categories and then to assign each review of some product to one of those categories. So, you know, so these are these sort of new semantic operators and what's... The main challenge is that when you include these in a database, optimizing for LLM processing cost becomes the bottleneck. Um, you know, suddenly a relational join is not a problem when you think about the fact that doing a semantic join is gonna involve N times M, uh, LLM invocations. And so your cost, your latency is gonna be dominated by that. And also, these semantic operators do not guarantee perfect quality. So now suddenly there's also this trade-off you have to balance between, "Well, maybe I'm gonna try to use smaller models to implement these operators. But if I do that, I might be sacrificing result quality." Um, so yeah, so it's a very interesting space. Uh, and I've worked on, uh, our own, at MIT, our own semantic query processing engine, um, Palimpsest, as well as its optimizer which was a separate paper we wrote called Abacus. And SemBench, uh, is this collaboration led by folks from Cornell, Google, uh, UTM, Bifold to Berlin, um, that is focused on trying to actually build sort of like the TPCH or TPC-DS benchmark for this setting. Uh, and so I played a small part in that, but, um, happy to talk about any and all of the above. 

00:05:46,010 --> 00:07:17,692 [Speaker 0]
Mm-hmm. Yeah, amazing. There's just so many cool ideas to it. I guess maybe, like, our journey of adding sort of the AI layer to querying databases at Weavia is we have kind of on one end our query engine that does this kinda like text-to-SQL thing, it can write filters and then... But that's one of the biggest innovations, I think, is just natural language to the operators. And then we have something that we call the transformation agent that's kind of like the semantic map where it applies an LLM over all the data in the database and then... But what we do is we then save the value into the database.And so, I think kind of the way that you use these operators is more like a femoral. Like, you're computing it on the fly, but you don't need to save it on the database. But that's just like a little... I just wanted to touch quickly on kind of like where we're at with Weaviate in respect to understanding these things. Um, and yeah, at, at first, um, I was following Liana Pattel's work after, um, she- she had came on the podcast. I met her and we talked about, uh, Acorn and, and so... And that's how I kind of became introduced to Lotus, and that's how I started to see this concept developing. Um, and so I think maybe if we could just kind of talk about the semantic operators. And, like, just... I know that you just gave a really great overview, but if we could just maybe even slow it down. I know in the SEM Bench you present these five operators of, um, uh, sorry, semantic filter, semantic join, uh, semantic classify, semantic map, and semantic rank. [laughs] I think I remembered all five of them in the paper. But, um, so maybe if we could just... I think, uh, uh, we could maybe just go further into those five and maybe explain how they differ. Part- particularly for me, I'm having a hard time understanding how semantic filter, semantic map, and semantic classifier really... They seem kind of similar to me. 

00:07:17,692 --> 00:07:17,812 [Speaker 1]
Yeah. 

00:07:17,812 --> 00:07:18,872 [Speaker 0]
Yeah. 

00:07:18,872 --> 00:12:30,243 [Speaker 1]
I think... So to some extent, I think this was an effort by the paper authors to try to identify what... 'Cause each of these systems has kind of adopted its own language of semantic operators, right? This is very early days. And so we're all trying to figure out, um, what is the space of operators. There's one view, which I guess you could call, like, the, the semantic map maximalist view, which says [laughs] that with a semantic map you can do all of semantic map, semantic filter, semantic classify, and semantic rank, right? Like, and... Well, actually that one is slightly different because it depends how you implement it. If you are taking an entire dataset and then saying, "I want to rank all the items in this dataset," then it's a bit more like an aggregate operation. Um, but if you were to first flatten, you know, things into a ro- like a row with the list, you could in theory use a map to just for that one row rank the items in the list. Um, so I do think that, to your point, a lot of them on the surface as a programmer you might say, "Oh, I could potentially implement both filter, classify, um, or rank using a map operator." I think that the key distinction and what's really important here, and I'll, I'll highlight this especially, uh, for map and filter 'cause I think this is where it's most obvious, is that by explicitly separating, you know, a filter from a map, you actually enable a different set of optimizations under the hood, um, that the semantic query processing engine can go and implement on the user's behalf. And I think that, um, you know, for database folks who are hearing this, this is kind of obvious. For AI folks who maybe don't think about declarative optimization so much, this might be like a little bit of a weird paradigm shift. Uh, and so just t- for like a 30-second overview of declarative optimization, the real pitch is that, you know, uh, in a database system or in a semantic query processing engine, you don't tell the system how it should implement an instruction, right? So, if you say, "I want to apply a semantic map to extract the summary of a research... of a set of research papers." Uh, so s- you know, extracting the summary for each paper in that dataset. You don't say, like, "You should do it using GPT-4.0 mini with this temperature setting." You don't say, "Oh, you should do it by using an ensemble of these three models and then synthesizing their summary into one more concise one." You just say, "I want to do a semantic map. This is the instruction. Go compute a summary." And the system is left to optimize that itself. And so you're putting your faith in the system to be able to take that high-level instruction, to compile it down to some physical execution that ideally is going to give you really high quality, really low cost, and really low latency. Um, and so because of this, uh, for semantic filter in particular, one way that a lot of these systems have optimized these filters, and I know this is true, uh, of Lotus, uh, and I think Thalamus DB, they also use approximate query processing techniques, so they may also be doing something somewhat similar. Um, but in Lotus, uh, what you effectively do is you have, let's say, your gold algorithm, which could just be, like, applying the most expensive model to all of the records in the dataset to compute that filter. And then you can have a proxy model, which is like a much cheaper and faster version of it. Uh, so that proxy model, instead of, let's say, being GPT-5, it could be, like, LLaMA, um, you know, 3.8 billion parameter model. And what they will effectively try to do for that filter operation is that they'll apply the Oracle model, the, the powerful model to a bunch of records. They'll then compute certain thresholds, where it's like, okay, anything above this threshold, um, for the- Sorry. They'll also run the proxy model on the same set. So they get sort of like a sense of, like, the proxy's confidence as well as the Oracle's confidence on how well each, uh, op- uh, record passes the filter. And then based on sort of the re- By looking at the result from the Oracle, and looking at the scores from the proxy, they can kind of set these thresholds where it's like, "Okay, if the proxy says something likely passes the filter with a score above, like, 0.7, let's just say, we're automatically gonna just let that pass the filter. We don't need to run it by the Oracle." If it's below some low threshold, we're gonna say, "Okay, it's automatically not gonna be processed by the filter." Uh, and it's... And then maybe if it's in some small band between the low and high threshold, then we say, "Okay, it's really unclear, so only these small band of records will go to the Oracle." And so in doing that, you effectively trade off potentially a little bit of accuracy by not using the Oracle for every, uh, record. But you save hopefully a lot in terms of processing cost and processing speed. The thing is, is that, um, you can do that for a filter, because it's very easy to assess the binary, like, correctness of the proxy and whether it agrees with the Oracle. Like, both of them said that this record passes the filter, or both of them said that this record does not pass the filter. Uh, with a semantic map where you're computing the summary of a research paper, you can't really do this. I can't take, like, the Oracle summary...... and the proxy summary and come up with some notion of like, "Oh, like, okay, in this setting, I can use that." So that's 

00:12:30,243 --> 00:12:51,964 [Speaker 1]
... To tie it all the way back to your question, that is why it's actually kind of important to have this distinction between a map and a filter, because the nuance of those operations allows for a different set of optimizations under the hood. Uh, and there's similar sort of things you can do with ranks and classify operators as well. So I think that that's kind of why we split out, uh, the space, the set of operators in this way. 

00:12:51,964 --> 00:13:00,804 [Speaker 0]
That's very cool. Yeah, there's so- so many cool things that, I mean, yeah, the kind of declarative programming model is always so exciting. I'm a huge DS Py fan, so I love to kind of- 

00:13:00,863 --> 00:13:01,214 [Speaker 1]
Yes, yes 

00:13:01,214 --> 00:13:54,304 [Speaker 0]
... think about it in that sense. Um, and yeah, uh, so I guess like with the operators, yeah, I think that's a r- a really great explanation of how semantic classify and semantic map, semantic filter differ for the query engine. Um, and then I kind of just... As, you know, listening to you talk is inspiring me thinking about these kind of database query planners, the kind of cost models, as you mentioned, the proxy model and the Oracle model. And so maybe we could introduce this concept and then later come back to semantic joins, because I think that is such an interesting different concept. But, um, this kind of like in relational algebra, say you have two filters, the car is red or the car is a BMW, there's like an order to which filter you should apply first. And so as I, as far as I understand it, that's kind of the general setup, is there's like an optimal ordering to how to apply the filters. And then maybe in Abacus, you have something like a sample 500, start running the filter in order to get the, um, like 

00:13:54,304 --> 00:13:54,554 [Speaker 2]
[clears throat] 

00:13:54,554 --> 00:14:03,983 [Speaker 0]
So- so does that even have a sense of what the cardinality of the resulting filters might be. And maybe you just mind explaining that sort of like what a query cost parting model is, and yeah. 

00:14:03,983 --> 00:17:51,304 [Speaker 1]
Totally. So, um, so I think that this is one of the, one of the strengths of, uh, of our work on Abacus. It's something that we, you know, we emphasize a lot. This is one distinction, I guess, between Lotus and Palimpsest and Abacus, um, which I'll- I'll just refer to that as either Abacus or PZ when talking about my work. Um, the name Palimpsest is kind of a mouthful. Um, but so Lotus uses like the, the data frame processing model. And so actually, it cannot do sort of like the filter reordering, like you mentioned. Um, and that's just because depending on the order in whi- ... Because it's not a lazy execution model, it means that for each line in your program, right? If you have your data frame and you do dataframe.filter, the car is red, and that returns a new dataframe. And then you do dataframe two is equal to dataframe.filter, the car is a BMW. The problem is you have to materialize that first dataframe that filters for the car as red before you can process the line that says the, you know, the dat- the car also is a BMW. And so in Lotus, you can't reorder those filters. Um, but in a system like Palimpsest, and I believe as well like in BigQuery and Thalamus DB, these, um, have a, a lazier, uh, execution model or just your traditional SQL model. So you submit basically the whole query, uh, and then what will happen is that underneath the hood, there is a query optimizer, which will take that query and then apply any set of, at least in Abacus, um, transformation and implementation rules. And so transformation rules include just like logical reorders to the plan. So that could include reordering filters. Um, it also in theory could include reordering joins, although we haven't, uh, written that up- optimization into our system yet. Um, and then it also includes what we call implementation rules. And so that's like saying, "I'm going to execute this filter with, uh, GPT-5," or, "I'm going to execute this filter with Gemini 2.5 Flash." So the implementation rules are kind of where you get your like, you know, uh, map or filter-specific optimization of like which model to use or, "Should I use an ensemble?" Things like that. Uh, and then each of these systems, to some extent, will have a cost model, which tries to effectively provide a prediction. Um, and it can be either, uh, in our case, it's based on sampling. But we try to get some notion for each operator of, uh, you know, if we implement it this way versus that way. What do we expect the cost per input is going to be? What is going to be the latency per input? And at- and at least in our system, on a scale of zero to one, how high quality is this operator going to perform, uh, for this specific map filter, uh, or classify operation? And then, um, basically, the- it's a dynamic programming algorithm that will go through all of the possible configurations of the physical plan, considering possible reorderings of the filters in the logical plan. Uh, and then it basically tries to return the physical plan and execute that one that is optimal for your user objective. And so- and that last part is also one, um, one distinction for our system, is in PZ and Abacus, uh, users can specify if they want to maximize quality, if they want to minimize cost, if they want to minimize latency. And they can also apply constraints to the other dimensions. So you can actually say, "For this query, I want to optimize for quality, but I don't want to spend more than like 10 cents per input record." Uh, and so that kind of gives you a little bit more control as a developer over, like, you know, if you're- if you're prototyping at first, you can just go for like the min-cost optimization until you get your query working. And then, over time, you can try and play around with like maximizing quality, but maybe with some thresholds on cost to keep it more manageable. 

00:17:51,304 --> 00:17:57,744 [Speaker 0]
Uh, I'd- I'd love to ask about how you think about with Palimpsest, the design of, um, 

00:17:57,744 --> 00:18:06,224 [Speaker 0]
uh, like... So does this, does this layer live as like a service outside of the database? Would- would this be something that's built directly into the database? 

00:18:06,224 --> 00:19:41,056 [Speaker 1]
Yeah, I think, um, so we build it as a, uh, a framework in Python. And we made that choice mainly because we were just like, "This is- this is where the AI folks are. This is very AI-driven." Um, at the time that we- we initially published the Palimpsest paper, like, uh, there were not many of these SQPs kind of around. I don't even think this sort of formalization had materialized yet. And so, uh...It was sort of like we wanted to do this outside of the database because we weren't sure whether the... Whether it was really gonna work that well. And it seemed like implementing this in a database was gonna be a lot of overhead and a lot of, um, you know, a lot of software engineering effort. So we implemented it outside so we could kinda have full control over the system, the optimizations. Uh, but I think that the... That all of the ideas in PZ and Abacus can be directly applied to a database. So like, um, you know, you could in theory apply this cost model or some of these optimizations into, like, uh, you know... A- and I mean Google's kind of proof of this in the Sam Bench paper, right? They have BigQuery, which has these semantic operators built into the system. Um, so yeah. So I think that you can... You can definitely... Like, PZ itself I wouldn't say is a database, um, but you can do many of the same operations in PZ. Um, and then if you were working at, you know, a database company, uh, but you wanted to borrow ideas from PZ, you wouldn't use PZ per se itself, but you would probably take... Lift those ideas and then implement them in your, uh, database. 

00:19:41,056 --> 00:19:48,546 [Speaker 0]
All right. That's super cool. And, uh, yeah, so, um, so I'm sorry if this is a dumb question, but I do- I don't really understand the kinda No, it's not a dumb question. 

00:19:48,546 --> 00:19:48,786 [Speaker 1]
... [laughs] 

00:19:48,786 --> 00:20:17,136 [Speaker 0]
[laughs] But, um, like, um, when you mention, like, the physical plan and the logical plan, I'm curious, like, uh, like, the... There's a physical plan. Like, when you're building palimpsest on top of a database like say Weaviate [laughs] or, or like PostgreSQL or whatever it is, d- can you get some kind of API to see, like, physically where is the data? Or is, is that what's meant by physical plan? Like, there is some... Here's exactly how on disk the data is, and these are the indexes or... Yeah. 

00:20:17,136 --> 00:20:42,496 [Speaker 1]
Kinda, yeah. It's, uh, it's inspired a lot by what you're saying, which is that the... So the logical plan is sort of just the representation of your query where you just have the types of the operators, right? So you can imagine, um, let's, let's take, uh, as an example, I think we have this diagram in our, uh, in our Abacus paper. Uh, there's a set of research papers, 

00:20:42,556 --> 00:22:50,676 [Speaker 1]
and you wanna filter for papers that are about databases. And then you wanna use a f- a first semantic map to summarize the main contributions of each paper. Uh, and then you wanna use a second semantic map to classify whether they're relevant to your interests, and, you know, you can specify what your interests are in the map instruction. All right. So there is a logical representation of that plan, uh, which we would say is just... It's a scan, 'cause you're gonna scan over all the research papers. And then it's a filter, so... 'Cause you're gonna apply your, you know, your filter, and then it's two logical maps. And so that logical plan can be optimized in essence by, um, making sure that the filter happens before the maps. You could imagine someone could have written the same program where they did, uh, you know, the two map operations, and then the last step was to filter about... This is about data systems, right? So logical optimizations for the logical plan correspond to things like, "Oh, I'm gonna push the filter down so that we execute that first." Um, it also... It can include, if you're... If you have semantic joins, you know, the order in which we execute these joins depending on, you know, what we think the cardinality of the join might be. Um, and then the physical optimization, in a logical plan, you still don't have this notion of like, "How am I gonna execute the map?" Like, "What model is actually going to do this task?" And so that's what... That's the difference between a logical and a physical plan. A physical plan is just saying like, "Okay, it's the logical plan where the filter comes first." It's a scan filter map map. "And specifically we're gonna do the first filter with like, you know, three... 8 billion parameter model." Uh, and then the second map with GPT-4o mini, and the final map is gonna be like some ensemble, um, that uses these three models. Uh, so yeah, so that's, that's the key distinction. Um, but to your point, in traditional databases, a lot of that difference between logical and physical is that, um, the physical plan depends a lot on understanding like where is data laid out. That's gonna tell you things like, uh... And like, "What indices do I have available to me?" That can give you a sense of like, "Okay, what's the cheapest scan option for this query?" So you're, you're definitely barking up the right tree. 

00:22:50,676 --> 00:23:18,426 [Speaker 0]
[laughs] Awesome. Yeah, well, it's super interesting. It... Yeah, it's such a cool topic. Um, I guess, I guess maybe just kind of before diving a little further into the systems and how particularly they optimize them and, you know, design these f- uh, especially talking about the selecting the models, I'm really excited to dive into that. But I also just maybe wanna set the stage for our listeners more on, uh, semantic joins. I'd seen this article really early on, like a... Here's, like, maybe an application of vector searching would be like [laughs] if you like dating, and so you'd have- 

00:23:18,426 --> 00:23:18,426 [Speaker 1]
Yeah 

00:23:18,426 --> 00:23:19,036 [Speaker 0]
... like the two 

00:23:19,036 --> 00:23:19,456 [Speaker 1]
[laughs] 

00:23:19,456 --> 00:23:24,296 [Speaker 0]
And you'd be... Yeah. Yeah. Do you mind, uh, maybe just telling the story of semantic joins? 

00:23:24,296 --> 00:28:40,132 [Speaker 1]
Yeah. Um, so semantic joins, I think there's a lot of, of interest in them, um, and excitement around them, in part because in traditional databases, uh, optimizing joins has been, uh, extremely important for getting good query performance. And, um, and I think that the... So I, I'm actually gonna get... Maybe I'm gonna be a bit of a letdown here. I'm gonna give you [laughs] a bit of a... Like, a pro and a con here. So the pro is that the same should be true in this... Uh, in the setting for, uh, semantic operators where op- you know, if you have a query with a semantic join, um, that is going to dominate the cost of the query. Uh, and like actually in the revision for Abacus, we just had to rewrite one of our, uh, programs to have three semantic joins. And one of these semantic joins involves like 12,000 images. And so the query costs like anywhere from $2 to $27 depending on how you physically implement, you know, the matching of images to text strings. Um, and it just shows you like how important it is to get... If you're trying to minimize cost, like use the cheaper embedding-based approach to join these things, 'cause like if you run a vision model on 12,000 images, you're gonna spend-... quite a lot of money. Um, so I think that from an academic perspective, um, there's a lot of excitement around, how do we optimize these joins? You can think of, uh, potentially another technique as being like, let's just say you wanted to, uh, join two different f- pairs of text. So maybe it's like movie titles and then descriptions of the movie. Like, do you know? Is this... Or, or genres perhaps. So for each movie you want to match, does it, uh, correspond to a certain genre? You don't necessarily have to, um, you know, take, let's say you have five movies and five, uh, genres. You could process them one by one. So you could take the first movie and the first genre and ask, or, you know, does the genre match the movie, true or false? And you could kind of then go down to the next genre and so on. Alternatively, you could also just take all five of them, feed them into the LLM as a batch, and then ask it to come up with a very compressed output that tells you only the movies that joined a certain genres. And so you would save on the number of LLM calls, which hopefully reduces your latency. Um, and also you could potentially save on some processing costs if you're clever about how you, uh, elicit the output from the model. Um, so there's... So for these reasons, like there's a lot of excitement around, you know, we've had all these ideas and we've spent a lot of time optimizing joins in relational databases. Clearly there's a lot of work and research to be done optimizing these joins for, uh, the semantic setting. So that's my, that's my bull, like my bull case. My bear case for semantic joins is that it's actually kind of hard to come up with... Um, it's not impossible, but it, it, it actually takes racking your brain a little bit sometimes to come up with interesting examples of semantic joins that are extremely relevant, um, for data processing workloads. Um, like, a- a- and maybe it's less true for text to text, um, but like for image to text even, um, it can be slightly difficult. Like, you might think of like, "Oh, does this caption match the image?" Okay, that makes sense, right? Yeah, like does this caption describe what's happening in the image? Totally get it. But like, what's an example of a setting where I have a bunch of captions and a bunch of images, but they're separated and I need to bring them together? Like, u- usually the two come together already. So, um, so I would say that... And I'll probably like be proven wrong in one year and there'll be like a lot of egg on my face for this take, and people will be like showing tons of examples and like, "Here's an obvious semantic join." Um, but at least that's one thing where sometimes when I, when I tell folks about this, one feedback that I have gotten from, uh, like non-AI experts in the past has been sort of like, "Okay, but like when am I gonna actually do that?" Um, or, "Why wouldn't I just, you know, apply a filter on both sides, extract some distinct field and then do a relational join?" And that is potentially an optimization also that's worth looking into, is like, uh, and, and that you could implement as an optimization for a semantic join in a system, would be to say like, "We actually do a, a filter and a map on both sides." Um, or sorry, "Do a map on both sides to extract like what animal is in this image, what animal is in this audio snippet, and then just join on those, uh, like those animals in a relational fashion." There are some downsides to it. We actually did kind of try this, um, for the animal image query. The problem is that sometimes on the map, um, if you don't like distinctly say like these are the classes for the animals, you can get one that's like, "This is an image of an elephant," and then you can get an audio recording like, "That's the audio of a North African elephant." And then when you try to join elephant to North African elephant, it doesn't join, if you do it relation- in a relational fashion. Um, so yeah. So it's a tricky, it's a tricky subject for sure, but I think it's one that's very exciting. Um, and I'm sure in a few years' time we'll have tons of use cases and I'll look like a fool for, uh, my skepticism, but yeah. 

00:28:40,132 --> 00:28:59,122 [Speaker 0]
Mm-hmm. And it starts me to think of one of those GitHub repositories, it's like awesome semantic joins, and it's just like a collection of like thousands of queries. Like there's like... I could imagine like the, um, the job applications to resumes one is like a good motivating example. I think, I do think the dating one is pretty good. We do have like a customer at Weaviate [laughs] that uses- 

00:28:59,122 --> 00:28:59,132 [Speaker 1]
Yeah 

00:28:59,132 --> 00:29:02,212 [Speaker 0]
... vector databases for that kind of thing. Um, but... [laughs] 

00:29:02,212 --> 00:29:50,812 [Speaker 1]
I think, I think it's, I think it's definitely, to your point, um, text to text, I think there's a lot of examples. Um, another one I s- I've heard about is there's a, a company that does, um, uh, sort of, sort of like Mechanical Turk, right? Like people submit tasks and then there are sort of workers, uh, with certain skills. And so they're actually trying to use, um, from what I understand, AI to try and match, you know, qualified, uh, people to perform given, uh, freelance tasks. Uh, and so, but again, the multimodality aspect I think is the, where it's kind of hard. It's like where do images, where do audio come into this? And so it may end up being the case, but even still, like if you can... If you have a lot of text things you need to join, it's good if you can do it in an optimized fashion. 

00:29:50,812 --> 00:29:58,051 [Speaker 0]
Yeah, that starts me to maybe think of joining like images with like music. Uh, yeah, that's cool. [laughs] 

00:29:58,052 --> 00:29:59,012 [Speaker 1]
Yeah, it's, it's- 

00:29:59,012 --> 00:29:59,172 [Speaker 0]
[laughs] 

00:29:59,172 --> 00:30:05,042 [Speaker 1]
Well, you see, it's a little tougher to come up with one and like convince yourself like, "Ah, yes, this is, this is, uh, needed." 

00:30:05,042 --> 00:30:05,051 [Speaker 0]
[laughs] 

00:30:05,052 --> 00:30:15,532 [Speaker 1]
But I'm sure that there are use cases. I just, you know, it's hard when you're in your bubble in academia sometimes to think of these things. But there are definitely folks in the industry who are like, "What are you talking about? Like I have to do this every day." 

00:30:15,532 --> 00:30:40,332 [Speaker 0]
[laughs] Yeah, it, um, it really... I mean, I, I love kind of setting the scene with like the declarative programming model and it's like how particularly we implement the semantic joins is indifferent from just defining it as one of the operators. But it makes me think like, it could be like either extreme m- you know, extreme multi-label classification to try to map these two things to each other. Like if I have 100, uh, unique categories of the one thing I'm trying to join with the other, it's like that problem. 

00:30:40,332 --> 00:30:40,832 [Speaker 1]
Right.

00:30:41,056 --> 00:30:59,456 [Speaker 0]
And then, uh, I really love, uh, topic modeling, uh, t- topic modeling where you're like clustering the vectors. It, it... You represent which object in the vector is, and then you cluster them, and then you summarize the clusters. And maybe that has something, 'cause you're kinda like com- coming up with the categories on the fly. I don't know if that's quite a semantic join. I think that's like a sum up, yeah. 

00:30:59,456 --> 00:31:02,216 [Speaker 1]
I think you're... Like a semantic group by potentially could be- 

00:31:02,216 --> 00:31:02,386 [Speaker 0]
Yeah 

00:31:02,386 --> 00:32:00,456 [Speaker 1]
... um, could be an instance of that. And the cool thing there is that actually what you just described would be a semantic group by, where the aggregation is also semantic. So you're, you're clustering the groups and then you're saying, "Okay, what does this group represent?" Like come up with a s- semantic description of it. Um, and I think that that is actually a very, very interesting problem because a lot of traditional, like, clustering work, um, you know, that's kind of what you're trying to do. It's like, if you have a bunch of unstructured data and you're just embedding it, clustering it, and then hoping that these clusters mean something. Um, in theory, there's a world in which we have... You just say, "I wanna do my semantic group by," uh, and the system can optimize it, and then you just can, you know, ideally have some levers of optimization to try and maybe get those clusters and groups to align better with what you are expecting in the data. Um, so it, in theory, could make that whole, like, embedding based clustering problem a lot more streamlined. 

00:32:00,516 --> 00:32:56,466 [Speaker 0]
Th- that... And also, like, out of opening the podcast I mentioned that at Weaviate we have this thing called Transformation Agent that's kinda like semantic map or semantic classify, where we save the attributes back into database, into the database. And something we saw people do a lot is they'd run the Transformation Agent and then they'd use the Query Agent to ask questions about this new property. Like, if you're saying the car is red and you didn't already have that stored. And so then with this kind of semantic group by, I was thinking like about, um, like Databricks has this catalo- uh, they call it a Unity Catalog, sorry, where it's like about cataloging data, and I thought that could be a really great, like, interface for this semantic group by, as like kinda like a catalog agent. In Weaviate, it's like something that would group your data, give you a nice way to visualize these groups. Um, yeah. Anyways, so it just... So cool. Um, I... So I'd love to... I think this will transition well into talking about the dataset 'cause we kinda talked about, like, these use cases, examples of semantic joins. I think kinda maybe... Yeah, if we could now kinda transition into talking about what SEMBench is and how you're trying to benchmark this. 

00:32:56,466 --> 00:36:49,736 [Speaker 1]
Yeah. Um, so I think that, uh, at a, at a high level, the goal for SEMBench is to kind of create a parallel to your TPC-H or TPC-DS, um, uh, benchmark, which for analytical query processing has kind of been like the gold standard for testing out, uh, different databases, different query engines, query optimizations. And so the key... But the key problem with, uh, those benchmarks is just that, um, they don't have, you know, text or like large text fields or images or audio. And so in this new domain where we have semantic operators and where cost, latency, and now variable quality are determined by the LLMs you use and how you apply them, um, it really requires a new benchmark so that we can test out systems and get a sense of, you know, um, e- Which systems perform well at what operators? Um, are there certain types of queries that are difficult across all systems or wherever? Do we see high variance? Uh, what optimizations work well for this setting? So, I think that the, the initial SEMBench paper, um, again, very large collaboration, in, of which I played a small part. So I, I wanna, you know, give credit to all the other authors on the paper. Uh, and I don't wanna speak for all of them, um, but just at least my perspective on the benchmark. Uh, and I think that it's... You know, it's a great addition to the space. We've already been able to benchmark, uh, Lotus, ThalamusDB, Palimpsest, and BigQuery, uh, in this setting, and create a leaderboard that's public so people can kind of investigate the results and, and play around with these different systems. And I think that, you know, there's probably a future where we're gonna continue to grow and expand the benchmark as well. Um, 'cause, you know, right now it has 55 queries across five different workloads. And so those, uh, workloads, we call them, like, movie, wildlife, e-commerce, MMQA, and medical. Um, and they span effectively, uh, queries over four data modalities: tabular data, text data, image, and audio. And they include all of the semantic operators that you just, uh, described. Not... Uh, They're not, uh, semantic group by and semantic aggregate, but also that's something that I think maybe in the future we would, we would look to include, uh, in the benchmark. Uh, just my perspective. Again, not speaking for everyone else. Uh, so yeah. So I think that the, the benchmark is great because for a while, um, you know, when we were writing both the Palimpsest and the Abacus paper, we were kind of... You kind of had to just like find datasets that had been used in other AI settings and papers and workloads and like... And usually we couldn't even use the task as it was described, so you had to kind of change it a little bit to make it fit really nicely into like, "Oh, this is actually a filter with a map, uh, and there's a..." And then maybe you can add a second filter somehow, so now we have to be good at figuring out the right ordering of filters. Um, or maybe I can represent this as a semantic join, and so now join reordering becomes a problem. Um, but there just, like, wasn't a clear and clean benchmark upon which to say, "Okay, we're all gonna evaluate on this. We're gonna have some agreement about what are the settings and what are the parameters for the different systems under evaluation, and get sort of a very clear understanding of which systems are good at, you know, good at which operations and, uh, how do they perform relative to one another." Um, and, you know, just... But I think that honestly, the relative performance is not- not that important, um, because again, it's very early. Um, these systems are still very sensitive also to prompts, um, and so you can get very large performance changes just based on changing the prompt, which, um, you know, ideally in the future you would normalize for that. Um, but yeah, I think it's, it's a step in the right direction, um, to... And it's hopefully the type of thing where when you build the right benchmark, a whole community can build up around that, uh, and, and sort of drive performance and drive research into this area.

00:36:51,086 --> 00:37:18,645 [Speaker 0]
Yeah, it's so cool. I think like database benchmarks is such like an evolving area. Like, um, as I mentioned, we're building this kinda like text-to-SQL thing. Um, we saw these datasets like Bird, Spider, uh, WikiSQL, but, but then we're like, "Well, we wanna add a search, uh, query operator to it," 'cause they don't include that in the SQL language. I can imagine adding all these semantic operators. And yeah, so, so, um, so, so you designed this dataset using Kaggle. Is that right? Uh, like, how do you... yeah. 

00:37:18,646 --> 00:39:18,306 [Speaker 1]
Yeah. So I think a lot of the, um... So for my part, I, I didn't, uh, do a lot of the, the query and benchmark creation. I was mostly responsible for just making sure that like the PZ queries were implemented correctly, helping support with bugs that came up in that sense. Um, but I do... My understanding is that for most of the scenarios, uh, what happened was they took publicly available Kaggle benchmarks, or Kaggle datasets, and then what they did was they created sort of 10 queries over those datasets. Um, so I think the animal one is actually a really, uh, useful example. Um, so there, I f- I actually think it was originally two separate datasets. If I'm... I... Don't quote me on this. Um, but I believe it was two separate datasets, one that had animal recordings and one that had images of animals, but they both had labels of which animal was in the recording or the image. And so what we were able to do is basically to construct a dataset where we said, "Okay, you know, we know the ground truth for what this recording should be in terms of the animal. We know the ground truth of, you know, what the image should be." And then we've also generated some tabular data about like the location where... And, and this was like ran- you know, generated at random somewhat. Like, the location of the image and the location of the recording. But then what that enables you to do is you can suddenly construct SQL queries that are like, um, you know, like, "Which location has the highest number of elephant images?" Um, and so that, you know, requires filtering over the images, but it also requires implementing an aggregation, a max operator over the output. And so, um, and so, yeah. So we were able to effectively construct these queries by s- taking existing Kaggle datasets. And the key reason we used Kaggle datasets was because they already had ground truth labels. So we have ground truth for all of the queries in the benchmark. Um, and that allows us to sort of fairly accurately assess, you know, what is the quality of a given operator. Uh, I think 

00:39:18,306 --> 00:40:03,266 [Speaker 1]
one limitation of this approach, or something that we'll have to explore in the future is, for something like a semantic map where you're trying to compute a summary, again, it would be very di- And maybe the answer is just that we don't do that in the benchmark. Like, we... If we do extraction of information, it has to be like very, um, well-defined and in a rigid sort of field. Um, so, you know, extracting like the authors of a paper. Like, that's... You know, we can very easily assess whether you did that correctly or not. Um, but I do think that there are... You know, anytime you're using LLMs, evaluating accuracy and quality is, is a challenge. Um, and in other settings, we've seen LLM judges be used, but, um, it... That's... You know, it remains to be seen what we might do for 

00:40:03,266 --> 00:40:10,666 [Speaker 1]
semantic operations that have less tangible outputs, something like a summary where it's harder to quantify. 

00:40:10,666 --> 00:41:00,746 [Speaker 0]
Yeah. M- mentioning adding the synthetic tabular data about the locations really reminds me of like, um... I mentioned kind of, uh, how I met Leon and sort of like just through the path of all these things, we came aware of the semantic operators. But, um, we were, we were researching, uh, filtered vector search, like vector searching with filters, and, and there's not a good dataset that, like, has the filters. So what we were doing is we would just, uh, do like mod 10 as we're inserting the data into the, into Weaviate, and that made... that's your filter, like one, two, three. And then, and then what we found is that there's actually... It, it makes sense in hindsight, 'cause like the feature, the filter, and then the vectors, the vector embedding correlation. But we found that there's like a correlation between the filter and the vector embedding that impacts the filtered vector. So, so it's just like you're trying to construct these benchmarks that like combine unstructured and structured. It's like I think created its own problem. Yeah. 

00:41:00,746 --> 00:41:38,445 [Speaker 1]
Yeah. It's very... It's, it's very time-consuming. Um, it's challenging work, um, but it needs to be done. And so I think that for people to, you know, public- Like, I think that publishing benchmark papers is a great, um, you know, a great direction for communities right now. Because like you said, even for a sort of hybrid vector search, um, we need to have really good authoritative... Not... You know, it shouldn't be the end-all, be-all, but you definitely do wanna have some sort of benchmark that everyone can kind of evaluate and get a sense of like, "Well, how well does my system perform? Where do I need to invest more energy and time, uh, to make my system better?" So yeah, I, I hope... totally agree. 

00:41:38,446 --> 00:42:14,546 [Speaker 0]
Yes. W- And, uh, so I think kind of this is re- You mentioned uh, kinda like the TPC benchmark, and this is something I really wanted to get your opinion on, is, um, the sort of interaction between semantic operators and then like difficult relational operators. So like, and kinda earlier I mentioned asking about like Palimpsest being built on top of other databases, and I'm cur- like, curious like, say, uh, like ClickHouse as a database is optimized for these like really fast analytical queries. And so I'm kinda... 'Cause like the, the... And you mentioned separating the logical from the physical, but like, certain databases implement certain non-semantic operators faster. How does that, how does that kind of interaction... huh? 

00:42:14,546 --> 00:42:24,705 [Speaker 1]
Yeah. Um, so I think that, um, the... Truthfully right now, for most of the systems [clears throat] and for most queries, 

00:42:24,706 --> 00:42:30,016 [Speaker 1]
you're... If all you do is focus on optimizing the semantic part, you're gonna do great. [laughs] 

00:42:30,016 --> 00:42:30,026 [Speaker 0]
[laughs] 

00:42:30,026 --> 00:43:16,690 [Speaker 1]
Like, like such a... Because the order of magnitude difference in processing, uh, time and cost is so, so large, even in a hybrid query where you do have a mix of relational and semantic operators, um, if all you really... If you fo- If your optimizer primarily focuses on just the semantic portion, uh, then you definitely can win on a lot of queries. That being said, you are 100% right. It is not impossible to construct datasets, uh-... with, let's say, millions of records, where potentially if you do the right thing on the relational side, you actually then only have to process very, very few semantic, uh, operators. And so you actually perhaps the, you know, the amount of compute that goes into doing some relational join or relational filter 

00:43:16,690 --> 00:44:18,430 [Speaker 1]
is so high that, um, it, it actually requires a, you know, intelligent optimization on the database side as well. Um, and so I think that that, um... I don't have a good answer for you right now about, like, what do the different systems do for this? I think most of them... Um, BigQuery, I would assume would probably be the most ahead right now because they, you know, they've optimized for relational queries for a very long time. Um, and so they probably still have access to those optimizations even when they're using, um, you know, their AI-enabled opera- semantic operators. Um, but yeah. In, in Palimpsest at least I can say we, you know, we don't do anything special for the relational, um, portion. Uh, but in the future if we have... I, one thing that I know we've talked about is adding indices into the system, uh, both in a semantic and non-semantic variety. So in theory if you had an index over some relational data, um, you know, we, we could leverage that instead of just doing some line scan or something like that. Um, yeah.

00:44:20,210 --> 00:44:23,190 [Speaker 1]
 Yeah. I've heard of... I guess, like, 

00:44:23,190 --> 00:44:38,750 [Speaker 0]
quickly talking about that, like, um, I think something we saw with, like, Weaviate is people would build a lot of rag systems where yeah, the vector search it might be only 10 milliseconds, and that's great, but the LLM is five seconds. So [laughs] it's like you saved me 90 milliseconds, but [laughs] yeah. 

00:44:38,750 --> 00:44:48,390 [Speaker 1]
Yeah. Yeah, that's, that's, that's just the nature of it. And I think, um... So one thing that I also am somewhat bullish on is that 

00:44:48,390 --> 00:46:31,290 [Speaker 1]
the processing cost and times are coming down. You have, obviously in the news, massive AI data centers being built, possibly propping up the entire US economy at this moment in time. Um, so this is a massive level of investment. And I think that everyone is hoping that, uh, three to four years from now, uh, you know, compute is gonna be so widely available that it may not be crazy to say that if I have a semantic filter, I could run 10,000 LLM operations in parallel. Right? So, you know, right now you're somewhat limited by, like, the API rate limit and how many, uh, you know, how many, you know, filter tasks you can fire off to some service provider. Or if your system is managing models, uh, on local hardware, right, it's like how many GPUs do you have? If the supply of GPUs goes up in both of those settings, then ideally we can parallelize these requests a lot more. Uh, and then so in that sense, um, the latency on the semantic side would hopefully come down, um, quite a bit. Uh, so, you know, again in this very straw man example of, like, I have 10,000 filters I want to do. If I have 10,000 invocations that I can do in parallel, well suddenly now at the time of this, it's the time for one, like, the max length LLM call in that batch. Um, and perhaps then, you know, your relational operators become much closer in terms of scale ordered magnitude in terms of time. So, I think definitely in the not too distant future you could see these things start to approach one another. Um, but right now, um, you know, GPUs are still somewhat scarce and so that, that dominates the bottleneck in computing, uh, these queries. 

00:46:31,350 --> 00:46:37,820 [Speaker 0]
Yeah, I think that's such a cool one. Uh, like our echo... I don't mean to be plugging Weaviate things like every conversation [laughs] 

00:46:37,820 --> 00:46:38,960 [Speaker 1]
No, no. 'Cause I like it. It's great. 

00:46:38,960 --> 00:47:03,350 [Speaker 0]
Just like what my intuition is taking me. But like we do that. Uh, and I'll give a shout-out to Modal. We use Modal as like really great zero to, I don't know about infinity, but like, you know, zero to tons of GPUs. And then that's how we do that, like, batch updating. And yeah, that, that kind of thing. Yeah. Uh, definitely a big to- I think like kind of like co-locating data in G- in compute is a very interesting topic with this. Um, but- 

00:47:03,350 --> 00:47:47,590 [Speaker 1]
I would... Yeah. Given infinite time, I would love to spend, um... So Palimpsest right now and Abacus, we mostly just support... We have very, very basic support for VLLM. So if you want to run, uh, you know, like a local model on some server. Um, but effectively all of the optimizations are not leveraging. Like, the system itself is controlling the model weights. And so we can do some fancy tricks there. That's 100% a great direction for future research, um, but something that is not currently in Palimpsest. Uh, but I think that there's a ton of, uh, room to explore how you could optimize in that setting. Um, but yeah. We... So yeah, we mostly focus kind of, uh, just on the API-based, uh, processing. 

00:47:47,590 --> 00:47:55,020 [Speaker 0]
Yeah. 'Cause you could have like a five-millisecond database index, but then a 50-millisecond network latency. So it's like... [laughs] 

00:47:55,020 --> 00:47:55,030 [Speaker 1]
Yeah. [laughs] 

00:47:55,030 --> 00:48:19,610 [Speaker 0]
And then you've got to co-locate the pods and the zones and [laughs] you know, it's... Uh, so, so yeah. So I really wanted to just sort of maybe anchor with, um... I, I don't know too much about the systems and the optimizations behind them, but maybe if you could just explain Palimpsest further, kind of like the, the landscape between Lotus and, uh, the Element DB and BigQuery again. And then just sort of like what exciting directions are sort of on the roadmap for Palimpsest? 

00:48:19,610 --> 00:51:22,018 [Speaker 1]
Yeah, totally. Um, so on the Palimpsest side of things, um, as I mentioned, you know, we support, uh, most of the s- most of the semantic operators, um, all the ones in SemBench, um, as well as a few others. And the primary means of optimization, uh, are that we... For maps and filters we sort of have four different optimizations. The first one is model selection, so which model am I gonna use to implement this map or filter? The second one is, uh, we call it mixture of agents. It's effectively an ensemble. Um, so you ask a bunch of what we call proposers to generate, you know, the map output or the filter decision. And then there is an aggregator which takes all of the proposer's outputs, and then synthesizes them into sort of one final, uh, either, you know, filter decision or, uh, map output.Uh, there is a critique-and-refine operator where an initial model proposes an answer, a second model then critiques the answer, and a third one produces the refined output. Um, and then the final optimization we have is sort of, we call it like context reduction. And so for that one, the simplest example is like if you wanna compute a summary of a paper's main contributions. You don't need to read all 12 pages of the paper. You can usually just read the introduction, and you should ideally get, you know, if the pa- if the paper authors did their job, from just the introduction, you can get a summary of the main contributions. And so in the context-reduction optimization, what we will do for filters and maps is, we'll actually first chunk the input, embed each chunk, take the embedding similarity with the map instruction or the filter instruction, and then take some top-K number of chunks or embeddings and only filter the map... apply the mapper filter using just those chunks. And we'll use like dot-dot-dot to connect the chunks. Um, and so out of everything I just described, model selection offers you the... offers Palimpsest and Abacus a way to explore the trade-off between latency, cost, and quality just on that thin fra- Pareto frontier of like, you know, which model you decide to use. The mixture of agents and the critique-and-refine are pretty much exclusively relative to model selection going to increase your cost, right? So now instead of using one model, we're using three in the critique-and-refine, and using some number of proposers and an aggregator in the ensemble. Uh, but what we do observe is that for large document processing tasks, uh, you do get better performance as well. Um, so in our Abacus paper, um, there is a task around like extracting, um, patients' reactions to drugs from these medical reports that describe, you know, a patient took XYZ drug and then they had this adverse reaction. Um, and so in that setting as well as in this other legal contract understanding, uh, benchmark called Quad, if you use these ensembles or these critique-and-refine loops, um, you can actually get higher quality. Um, again though at the trade-off of higher cost and higher latency. 

00:51:22,018 --> 00:54:30,658 [Speaker 1]
And then the context-reduction one is primarily geared towards saving you on cost and on latency. So if you have a large input document, um, by chunking it and only processing your map or filter with the most relevant chunks, we can potentially save a significant amount of input token processing, um, you know, by ignoring most of the irrelevant portions of the document. So that's on the filter and map side. Uh, we have a top-K operator which is sort of, um, like we effectively will retrieve the top K objects from some vector database, um, based on their relevance to some, uh, some natural language, uh, texturing. Uh, and so we can optimize for the value K of how many objects do you want returned. So effectively you can think of it as like if you have some pipeline that's doing some sort of RAG operation, we can potentially optimize the value K for what you return. And, you know, ideally small K is gonna give you obviously fewer items, um, but also could avoid cluttering the context. So it's, it's a little less clean in terms of like is it saving you on cost? Is it improving on quality? Um, but the nice thing is that we at least give the optimizer the ability to try and figure out like what's the optimal value of K for your task and your workload. Um, and then the final one is semantic joins. And so we offer a, uh, the naive nested-loops join, which is just take every pair of items and run an LLM. Um, but we also have an embedding-based approach, uh, in which we compute a embedding for every item, and then we use aga- like an oracle model on some subset of the data to come up with that high and low threshold. And then for the rest of the query, you know, anything where the embedding pair similarity is above the high threshold, we automatically join it. Below the low threshold, we filter it out. And if it's in that middle band, then we still use the oracle. Um, yeah, so that's like sort of the overview landscape of optimizations in Palimpsest and Abacus. And the way that the system actually determines what... which optimizations to apply is that the... there's like sort of a fixed-sample budget which can be specified in dollar cost now. So you can say like, "I'll spend f- f- you know, a dollar on LLM calls to try out a bunch of these different techniques." Uh, and we have some like clever algorithms around trying to pick promising operators and early, and then continue to sample those operators to get better and better estimates about their performance. Um, and then once the sampling budget is exhausted, then we do that sort of, as I was saying, like that logical-to-physical plan construction, generating a very large base of physical plans using all the operators that we sampled, and all the different approaches to implementing the map, the filter, the join, top K, uh, and then returning sort of the Pareto-optimal plan for the user's objective. So that's, um... And usually when I say that all, there's like a very nice slide behind me that I can like click through and point at. Um, so I don't know how well that comes across if I just say it out loud. Um, but feel... Like if you have any questions about that, please ask me so I can... I can help clarify. 

00:54:30,658 --> 00:54:36,138 [Speaker 0]
[laughs] Yeah, it's, it's so cool. So many exciting ideas are just, yeah, hard to even pick a question- 

00:54:36,138 --> 00:54:36,188 [Speaker 1]
Of course 

00:54:36,188 --> 00:54:57,968 [Speaker 0]
... 'cause there's so many cool things to explore. But, uh, I just... I guess I had a quick question about like sort of the implementation of semantic, uh, filtering or classify like, is that like... it could just be an LLM inference or it could itself be a RAG inference. Or, or I guess it could be like an agentic inference where you use like a function calling loop and it's got tools to implement like each of the calls? Um, is that... 

00:54:57,968 --> 00:54:57,977 [Speaker 1]
Yeah. 

00:54:57,978 --> 00:54:59,238 [Speaker 0]
Is that what... Yeah. 

00:54:59,238 --> 00:58:48,786 [Speaker 1]
Yeah, I... we... I mean the spa- like the s- potential space of... and I think part of what... so part of what drove our interest in this direction that we took for Abacus was that we imagined that the space of optimizations is...... is cra- is insanely large. Um, and some of the other s- semantic query processing engines, uh, again... And this is like... This is our difference because this is sort of our very biased view of the way, of the way that these engines should work. So naturally, this means that our system... Like, this is the line on which they differ from others. Um, in a lot of other systems, there's sort of this bias put on the, the... in terms of the people who wrote the system where they say like, there's w- a certain goal, which is like, "We're gonna try to preserve the quality of some model, but potentially save cost by using like a proxy to process some amount of the data." But what that really kind of restricts the system too is like, it can only optimize for saving on cost and quality relative to some oracle. And if that oracle is not great, then you're kind of... Like, there's not really much that you can do in tur- in the system internally to optimize and make the query better. Um, and one thing that we wanted in Abacus was to admit as the largest spaces possible of these types of optimizations, and provide effectively an interface in which developers can write new optimizations. And by design, our optimizer treats each optimization as a black box. That's why it has to sort of sample them to get an estimate of how well they perform. Um, but because of this, basically what that means is that as long as you... Like, let's say that you decide that we should try doing semantic filtering, um, by first doing RAG to look up... Like, embed everything that you're gonna filter over. Use RAG to get the top K equal to 100, and maybe K is a hyperparameter so it can be scaled, and then you only process like those K items with the LLM. And that optimization, again, the lever is that value of K. So like if K is one, it's a super cheap operation, but it probably misses a lot of important things. If K is the entire dataset, well then you're just running an LLM on everything at the end of the day, so it's super expensive. But you can tune this K parameter to kind of trade off between cost and hopefully accuracy. You can go write that in a class. You just have to implement two functions. You can then plug it into Palimpsest, and Abacus will basically be able to optimize over it. Um, again, because it sort of treats every optimization as a black box that has to be sampled in order to assess its, its quality, its cost, and its latency. Um, so we definitely believe that, um, you know, this admits a lot of fancy optimizations. Like in theory, you could even have an optimization where the optimization is literally like an agent will write Python code, and then apply that Python code to do the filter operation. Uh, and perhaps in the Python code, it's invoking like some hugging face model, or if it's like simple enough of a filter, it can just be implemented with a regular expression or, uh, you know, other, like, string-matching logic. That's also possible. And then the great thing there is like, you write the function once, that's one LLM call, but then you apply the function with the rest of the dataset so you save a ton on, on your cost. And th- that's sort of the key, um, like biased perspective for the Palimpsest and Abacus system, is like you could go and write that optimization today, plug it into our system, and it would work. Um, you know, a- and, uh, so we're not, we're not being prescriptive about like what optimizations should exist in the system. We're not curating that. We're admitting the widest possible space, and then saying that we're gonna try... And our, our secret sauce is like, "Can we search over a massive space of these optimizations and find ones which are good for your court?" 

00:58:48,786 --> 00:59:33,585 [Speaker 0]
Yeah, that's so cool. And that, and that has made me think about like kind of the integration of DSPy where you might submit a prompt for like a t- one of these tasks, and then you optimize that prompt with Gepard as one of the optimizations you're exploring in parallel. And yeah, it's, it's really amazing. And then the systems thing, mentioning like each of the inferences is a RAG inference, now you maybe have this like batch query workload. And it's kind of something like when we were working with NVIDIA, they have like the CAGRA vector index for like running on GPUs and the... And so it's like... It's really particular... It's really particularly really good at building the v- proximity graph really quickly and then batch queries. And so I just always kind of been thinking now about like, oh, what is the application of like batch queries? And maybe this is that example because you're about to run 100,000 [laughs] RAG inferences. [laughs] 

00:59:33,585 --> 00:59:34,935 [Speaker 1]
Yeah, right. 

00:59:34,935 --> 00:59:35,705 [Speaker 0]
Or, or... [laughs] 

00:59:35,705 --> 00:59:36,136 [Speaker 1]
Yeah, yeah. 

00:59:36,136 --> 00:59:36,316 [Speaker 0]
Yeah. 

00:59:36,316 --> 01:00:01,066 [Speaker 1]
So it's, um, yeah. But it's a, it's an exciting space, um, and I think, uh... And that's, and that's one of the key things, is that the space of optimizations, because AI development happens so quickly, it's just gonna grow and grow. Um, and so we're, we're designing to try and be flexible, um, to admit new optimizations and new techniques as we get more great new papers about all these different AI processing techniques. 

01:00:01,066 --> 01:00:20,586 [Speaker 0]
Yeah, amazing. Well, Matthew, thanks so much for joining the VB podcast. I've learned so much about semantic query processing engines and all these things are just so exciting. Uh, one, one sort of anchoring question I'd like to ask, just maybe like broadly outside of your particular expertise in this work, is just sort of like what future directions for AI excite you the most? Like, what kind of gets you out of bed in the morning thinking about AI? 

01:00:20,586 --> 01:00:27,696 [Speaker 1]
Yeah. Um, I think, uh, I guess one very succinct one for me is, is deep research. Um, and so- 

01:00:27,696 --> 01:00:27,696 [Speaker 0]
Mm-hmm 

01:00:27,696 --> 01:00:37,086 [Speaker 1]
... I've been really... Because, uh, at a very high level, deep research actually looks a lot like a declarative 

01:00:37,086 --> 01:01:17,236 [Speaker 1]
database processing system. Like, users give very high level intent. The system then has to take that intent, come up with some plan for how it's gonna, you know, answer the question that the user provided or do the task that the user wants it to do. Um, and then, you know, after 20 or 30 minutes, I think for current sy- systems, like, maybe less time, but, y- you know, the system will come back with your final output. But that maps very closely to this declarative world in which you say like, the user doesn't tell the deep research agent, "These are the steps you have to follow." They just say like, "Here's some Excel spreadsheets," like, "Come up with a financial model for investing in Fortune 500 companies," or something like that. Or like, you know- 

01:01:17,236 --> 01:01:17,236 [Speaker 0]
[laughs] 

01:01:17,236 --> 01:01:21,955 [Speaker 1]
... we hope students don't do this, but I am sure students have done this, like, "Here's a chapter of my textbook." 

01:01:21,955 --> 01:01:21,966 [Speaker 0]
[laughs] 

01:01:21,966 --> 01:01:47,306 [Speaker 1]
Like, "Generate a study plan [laughs] for me." Um, and so, you know, what users care about is like, "I have this instruction, I want the output," and it's up to the system to optimize, like, how do you go from instruction to output? So I'm excited by that 'cause I think that it, it maps pretty closely to this space, and i- it remains to be seen what's gonna be the overlap or the parallels between semantic query processing engines and deep research. 

01:01:47,306 --> 01:01:54,045 [Speaker 0]
Yeah, I, I think maybe the generator study plan for the student [laughs] is such a, is such a bad use case of it. [laughs] 

01:01:54,046 --> 01:01:57,045 [Speaker 1]
[laughs] It's definitely in demand, I can say that. 

01:01:57,046 --> 01:02:05,346 [Speaker 0]
[laughs] But like, the, um, you know, get my, my tweet, get this a million impressions, whatever you have to do, or... [laughs] 

01:02:05,346 --> 01:02:09,246 [Speaker 1]
Yeah. Well, the, the opportunities for optimization are, are boundless. So that's good. 

01:02:09,246 --> 01:02:09,906 [Speaker 0]
[laughs] 

01:02:09,906 --> 01:02:12,145 [Speaker 1]
That's good for me as a researcher. 

01:02:12,145 --> 01:02:24,286 [Speaker 0]
[laughs] Yeah, amazing. Well, Matthew, thanks again for joining. Uh, just amazing, really loved learning about all these things from you. And yeah, just really excited to keep up with the progress of this. It's such an exciting field. Thanks so much. 

01:02:24,286 --> 01:02:25,806 [Speaker 1]
Thank you. Bye.